---
description: "Comprehensive Apache Kafka message streaming standards with producer/consumer patterns, cluster management, topic design, security policies, and production deployment following distributed systems best practices"
globs: ["**/kafka/**/*", "**/confluent/**/*", "**/streams/**/*", "**/connect/**/*", "**/schema-registry/**/*"]
alwaysApply: false
---

<rule>
  <meta>
    <title>Kafka Core Standards</title>
    <description>Comprehensive Apache Kafka message streaming standards covering producer/consumer patterns, cluster management, topic design, security policies, monitoring, and production deployment following distributed systems and event-driven architecture best practices</description>
    <created-at utc-timestamp="1744157700">January 25, 2025, 10:15 AM</created-at>
    <last-updated-at utc-timestamp="1744157700">January 25, 2025, 10:15 AM</last-updated-at>
    <applies-to>
      <file-matcher glob="**/kafka/**/*">Kafka-related configuration and client files</file-matcher>
      <file-matcher glob="**/confluent/**/*">Confluent Platform configuration files</file-matcher>
      <file-matcher glob="**/streams/**/*">Kafka Streams application files</file-matcher>
      <file-matcher glob="**/*kafka*">Kafka-related files throughout the application</file-matcher>
      <file-matcher glob="**/producer/**/*">Kafka producer implementation files</file-matcher>
      <file-matcher glob="**/consumer/**/*">Kafka consumer implementation files</file-matcher>
      <action-matcher action="message-streaming">Triggered when working with Kafka message streaming</action-matcher>
    </applies-to>
  </meta>
  <requirements>
    <non-negotiable priority="critical">
      <description>Use Kafka with proper cluster configuration, security policies, topic design, producer/consumer patterns, monitoring, and error handling. Implement comprehensive authentication, authorization, encryption, and production-ready deployment strategies with proper observability and disaster recovery.</description>
      <examples>
        <example title="Production Kafka Cluster Configuration">
          <correct-example title="Secure, scalable Kafka cluster setup" conditions="Deploying production Kafka cluster" expected-result="Secure, highly available Kafka deployment" correctness-criteria="Security configuration, high availability, monitoring, proper resource allocation, backup strategies"><![CDATA[#!/bin/bash
# kafka-cluster-setup.sh - Production Kafka cluster deployment

set -euo pipefail

echo "🚀 Setting up production Kafka cluster"

# Kafka cluster configuration - server.properties
cat > server.properties <<EOF
# Broker configuration
broker.id=1
listeners=PLAINTEXT://0.0.0.0:9092,SSL://0.0.0.0:9093,SASL_SSL://0.0.0.0:9094
advertised.listeners=PLAINTEXT://kafka-1.internal:9092,SSL://kafka-1.external:9093,SASL_SSL://kafka-1.secure:9094
listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_SSL:SASL_SSL

# Security configuration
security.inter.broker.protocol=SASL_SSL
sasl.mechanism.inter.broker.protocol=PLAIN
sasl.enabled.mechanisms=PLAIN,SCRAM-SHA-256,SCRAM-SHA-512

# SSL configuration
ssl.keystore.location=/etc/kafka/ssl/kafka.server.keystore.jks
ssl.keystore.password=changeit
ssl.key.password=changeit
ssl.truststore.location=/etc/kafka/ssl/kafka.server.truststore.jks
ssl.truststore.password=changeit
ssl.client.auth=required
ssl.enabled.protocols=TLSv1.2,TLSv1.3
ssl.cipher.suites=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256

# Authorization
authorizer.class.name=kafka.security.authorizer.AclAuthorizer
allow.everyone.if.no.acl.found=false
super.users=User:admin;User:kafka

# Data directories and replication
log.dirs=/var/kafka-logs
num.network.threads=8
num.io.threads=16
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600

# Topic configuration
default.replication.factor=3
min.insync.replicas=2
unclean.leader.election.enable=false
auto.create.topics.enable=false

# Log configuration
log.retention.hours=168
log.retention.bytes=1073741824
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000
log.cleanup.policy=delete

# Zookeeper configuration
zookeeper.connect=zk-1:2181,zk-2:2181,zk-3:2181/kafka
zookeeper.connection.timeout.ms=18000
zookeeper.session.timeout.ms=18000

# Performance tuning
num.partitions=12
num.recovery.threads.per.data.dir=1
offsets.topic.replication.factor=3
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=2

# JVM configuration
KAFKA_HEAP_OPTS="-Xmx6g -Xms6g"
KAFKA_JVM_PERFORMANCE_OPTS="-server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -Djava.awt.headless=true"

# Monitoring and metrics
metric.reporters=io.confluent.metrics.reporter.ConfluentMetricsReporter
confluent.metrics.reporter.bootstrap.servers=kafka-1:9092,kafka-2:9092,kafka-3:9092
confluent.metrics.reporter.topic.replicas=3
confluent.metrics.enable=true

EOF

# Docker Compose for Kafka cluster
cat > docker-compose.kafka.yml <<EOF
version: '3.8'

services:
  # Zookeeper ensemble
  zookeeper-1:
    image: confluentinc/cp-zookeeper:latest
    hostname: zookeeper-1
    container_name: zookeeper-1
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888
      ZOOKEEPER_SERVER_ID: 1
    volumes:
      - zk1-data:/var/lib/zookeeper/data
      - zk1-logs:/var/lib/zookeeper/log
    networks:
      - kafka-network

  zookeeper-2:
    image: confluentinc/cp-zookeeper:latest
    hostname: zookeeper-2
    container_name: zookeeper-2
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888
      ZOOKEEPER_SERVER_ID: 2
    volumes:
      - zk2-data:/var/lib/zookeeper/data
      - zk2-logs:/var/lib/zookeeper/log
    networks:
      - kafka-network

  zookeeper-3:
    image: confluentinc/cp-zookeeper:latest
    hostname: zookeeper-3
    container_name: zookeeper-3
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888
      ZOOKEEPER_SERVER_ID: 3
    volumes:
      - zk3-data:/var/lib/zookeeper/data
      - zk3-logs:/var/lib/zookeeper/log
    networks:
      - kafka-network

  # Kafka brokers
  kafka-1:
    image: confluentinc/cp-kafka:latest
    hostname: kafka-1
    container_name: kafka-1
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,EXTERNAL://0.0.0.0:19092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-1:9092,EXTERNAL://localhost:19092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 12
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 1073741824
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"
      KAFKA_JVM_PERFORMANCE_OPTS: "-server -XX:+UseG1GC -XX:MaxGCPauseMillis=20"
    volumes:
      - kafka1-data:/var/lib/kafka/data
    ports:
      - "19092:19092"
    networks:
      - kafka-network

  kafka-2:
    image: confluentinc/cp-kafka:latest
    hostname: kafka-2
    container_name: kafka-2
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,EXTERNAL://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-2:9092,EXTERNAL://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 12
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 1073741824
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"
      KAFKA_JVM_PERFORMANCE_OPTS: "-server -XX:+UseG1GC -XX:MaxGCPauseMillis=20"
    volumes:
      - kafka2-data:/var/lib/kafka/data
    ports:
      - "29092:29092"
    networks:
      - kafka-network

  kafka-3:
    image: confluentinc/cp-kafka:latest
    hostname: kafka-3
    container_name: kafka-3
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,EXTERNAL://0.0.0.0:39092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-3:9092,EXTERNAL://localhost:39092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 12
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 1073741824
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"
      KAFKA_JVM_PERFORMANCE_OPTS: "-server -XX:+UseG1GC -XX:MaxGCPauseMillis=20"
    volumes:
      - kafka3-data:/var/lib/kafka/data
    ports:
      - "39092:39092"
    networks:
      - kafka-network

  # Schema Registry
  schema-registry:
    image: confluentinc/cp-schema-registry:latest
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka-1:9092,kafka-2:9092,kafka-3:9092
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: 3
    ports:
      - "8081:8081"
    networks:
      - kafka-network

  # Kafka Connect
  kafka-connect:
    image: confluentinc/cp-kafka-connect:latest
    hostname: connect
    container_name: connect
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
      - schema-registry
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka-1:9092,kafka-2:9092,kafka-3:9092
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: '[%d] %p %X{connector.context}%m (%c:%L)%n'
    ports:
      - "8083:8083"
    networks:
      - kafka-network

volumes:
  zk1-data:
  zk1-logs:
  zk2-data:
  zk2-logs:
  zk3-data:
  zk3-logs:
  kafka1-data:
  kafka2-data:
  kafka3-data:

networks:
  kafka-network:
    driver: bridge

EOF

echo "✅ Kafka cluster configuration completed"]]></correct-example>
          <incorrect-example title="Basic Kafka setup without security or high availability" conditions="Setting up Kafka cluster" expected-result="Secure, production-ready Kafka deployment" incorrectness-criteria="No security, single node, no monitoring, poor configuration"><![CDATA[version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
    ports:
      - "9092:9092"

# Bad: Single node setup
# Bad: No security configuration
# Bad: No authentication or authorization
# Bad: No SSL/TLS encryption
# Bad: No monitoring
# Bad: No resource limits
# Bad: No data persistence]]></incorrect-example>
        </example>
      </examples>
    </non-negotiable>

    <requirement priority="critical">
      <description>Implement robust producer and consumer patterns with proper error handling, retry mechanisms, idempotency, exactly-once semantics, and comprehensive monitoring for reliable message processing.</description>
      <examples>
        <example title="Production Kafka Producer/Consumer Implementation">
          <correct-example title="Robust producer and consumer with error handling and monitoring" conditions="Implementing Kafka producers and consumers" expected-result="Reliable message processing with comprehensive error handling" correctness-criteria="Error handling, retry mechanisms, monitoring, idempotency, exactly-once semantics"><![CDATA[// TypeScript - Production Kafka Producer Implementation
import { Kafka, Producer, ProducerRecord, CompressionTypes } from 'kafkajs';
import { createLogger } from 'winston';
import { Counter, Histogram, register } from 'prom-client';
import { createHash } from 'crypto';

interface KafkaProducerConfig {
  clientId: string;
  brokers: string[];
  ssl?: boolean;
  sasl?: {
    mechanism: 'plain' | 'scram-sha-256' | 'scram-sha-512';
    username: string;
    password: string;
  };
  retry?: {
    retries: number;
    initialRetryTime: number;
    maxRetryTime: number;
  };
  compression?: CompressionTypes;
  idempotent?: boolean;
  transactionTimeout?: number;
}

interface MessageMetadata {
  messageId: string;
  timestamp: number;
  source: string;
  version: string;
  correlationId?: string;
  causationId?: string;
}

interface ProducerMetrics {
  messagesProduced: Counter<string>;
  messageSize: Histogram<string>;
  producerLatency: Histogram<string>;
  producerErrors: Counter<string>;
}

class KafkaProducerService {
  private kafka: Kafka;
  private producer: Producer;
  private logger = createLogger({ level: 'info' });
  private metrics: ProducerMetrics;
  private isConnected = false;

  constructor(private config: KafkaProducerConfig) {
    this.kafka = new Kafka({
      clientId: config.clientId,
      brokers: config.brokers,
      ssl: config.ssl,
      sasl: config.sasl,
      retry: {
        retries: config.retry?.retries || 8,
        initialRetryTime: config.retry?.initialRetryTime || 100,
        maxRetryTime: config.retry?.maxRetryTime || 30000,
      },
      connectionTimeout: 3000,
      requestTimeout: 30000,
    });

    this.producer = this.kafka.producer({
      maxInFlightRequests: 1, // Ensure ordering
      idempotent: config.idempotent || true,
      transactionTimeout: config.transactionTimeout || 30000,
      retry: {
        retries: 5,
        initialRetryTime: 300,
        maxRetryTime: 30000,
        factor: 2,
        multiplier: 2,
      },
    });

    this.initializeMetrics();
    this.setupErrorHandling();
  }

  private initializeMetrics(): void {
    this.metrics = {
      messagesProduced: new Counter({
        name: 'kafka_messages_produced_total',
        help: 'Total number of messages produced',
        labelNames: ['topic', 'partition', 'status'],
      }),
      messageSize: new Histogram({
        name: 'kafka_message_size_bytes',
        help: 'Size of produced messages in bytes',
        labelNames: ['topic'],
        buckets: [100, 1000, 10000, 100000, 1000000],
      }),
      producerLatency: new Histogram({
        name: 'kafka_producer_latency_seconds',
        help: 'Producer latency in seconds',
        labelNames: ['topic'],
        buckets: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5],
      }),
      producerErrors: new Counter({
        name: 'kafka_producer_errors_total',
        help: 'Total number of producer errors',
        labelNames: ['topic', 'error_type'],
      }),
    };

    register.registerMetric(this.metrics.messagesProduced);
    register.registerMetric(this.metrics.messageSize);
    register.registerMetric(this.metrics.producerLatency);
    register.registerMetric(this.metrics.producerErrors);
  }

  private setupErrorHandling(): void {
    this.producer.on('producer.connect', () => {
      this.logger.info('Producer connected');
      this.isConnected = true;
    });

    this.producer.on('producer.disconnect', () => {
      this.logger.warn('Producer disconnected');
      this.isConnected = false;
    });

    this.producer.on('producer.network.request_timeout', (payload) => {
      this.logger.error('Producer request timeout', payload);
      this.metrics.producerErrors.inc({ topic: 'unknown', error_type: 'timeout' });
    });
  }

  async connect(): Promise<void> {
    try {
      await this.producer.connect();
      this.logger.info('Kafka producer connected successfully');
    } catch (error) {
      this.logger.error('Failed to connect producer', error);
      throw error;
    }
  }

  async disconnect(): Promise<void> {
    try {
      await this.producer.disconnect();
      this.logger.info('Kafka producer disconnected successfully');
    } catch (error) {
      this.logger.error('Failed to disconnect producer', error);
      throw error;
    }
  }

  async sendMessage<T>(
    topic: string,
    message: T,
    options: {
      key?: string;
      partition?: number;
      headers?: Record<string, string>;
      metadata?: Partial<MessageMetadata>;
    } = {}
  ): Promise<void> {
    const startTime = Date.now();
    
    try {
      if (!this.isConnected) {
        throw new Error('Producer is not connected');
      }

      const metadata: MessageMetadata = {
        messageId: this.generateMessageId(),
        timestamp: Date.now(),
        source: this.config.clientId,
        version: '1.0.0',
        ...options.metadata,
      };

      const messageWithMetadata = {
        data: message,
        metadata,
      };

      const serializedMessage = JSON.stringify(messageWithMetadata);
      const messageSize = Buffer.byteLength(serializedMessage, 'utf8');

      const record: ProducerRecord = {
        topic,
        messages: [
          {
            key: options.key,
            value: serializedMessage,
            partition: options.partition,
            headers: {
              ...options.headers,
              'content-type': 'application/json',
              'message-id': metadata.messageId,
              'timestamp': metadata.timestamp.toString(),
            },
          },
        ],
      };

      const result = await this.producer.send(record);
      
      const latency = (Date.now() - startTime) / 1000;
      
      // Update metrics
      this.metrics.messagesProduced.inc({ 
        topic, 
        partition: result[0].partition.toString(), 
        status: 'success' 
      });
      this.metrics.messageSize.observe({ topic }, messageSize);
      this.metrics.producerLatency.observe({ topic }, latency);

      this.logger.info('Message sent successfully', {
        topic,
        partition: result[0].partition,
        offset: result[0].baseOffset,
        messageId: metadata.messageId,
        latency,
      });

    } catch (error) {
      const latency = (Date.now() - startTime) / 1000;
      
      this.metrics.producerErrors.inc({ 
        topic, 
        error_type: error.name || 'unknown' 
      });
      
      this.logger.error('Failed to send message', {
        topic,
        error: error.message,
        latency,
      });
      
      throw error;
    }
  }

  async sendBatch<T>(
    topic: string,
    messages: Array<{
      data: T;
      key?: string;
      partition?: number;
      headers?: Record<string, string>;
    }>
  ): Promise<void> {
    const startTime = Date.now();
    
    try {
      const recordMessages = messages.map((msg) => {
        const metadata: MessageMetadata = {
          messageId: this.generateMessageId(),
          timestamp: Date.now(),
          source: this.config.clientId,
          version: '1.0.0',
        };

        const messageWithMetadata = {
          data: msg.data,
          metadata,
        };

        return {
          key: msg.key,
          value: JSON.stringify(messageWithMetadata),
          partition: msg.partition,
          headers: {
            ...msg.headers,
            'content-type': 'application/json',
            'message-id': metadata.messageId,
            'timestamp': metadata.timestamp.toString(),
          },
        };
      });

      const record: ProducerRecord = {
        topic,
        messages: recordMessages,
      };

      const results = await this.producer.send(record);
      
      const latency = (Date.now() - startTime) / 1000;
      
      results.forEach((result) => {
        this.metrics.messagesProduced.inc({ 
          topic, 
          partition: result.partition.toString(), 
          status: 'success' 
        });
      });

      this.logger.info('Batch sent successfully', {
        topic,
        messageCount: messages.length,
        latency,
      });

    } catch (error) {
      this.metrics.producerErrors.inc({ 
        topic, 
        error_type: error.name || 'unknown' 
      });
      
      this.logger.error('Failed to send batch', {
        topic,
        messageCount: messages.length,
        error: error.message,
      });
      
      throw error;
    }
  }

  private generateMessageId(): string {
    const timestamp = Date.now();
    const random = Math.random().toString(36).substring(2);
    return createHash('sha256')
      .update(`${timestamp}-${random}-${this.config.clientId}`)
      .digest('hex')
      .substring(0, 16);
  }

  // Health check method
  async healthCheck(): Promise<{ status: string; connected: boolean }> {
    return {
      status: this.isConnected ? 'healthy' : 'unhealthy',
      connected: this.isConnected,
    };
  }
}

// Kafka Consumer Implementation
import { Consumer, EachMessagePayload, PartitionAssigner } from 'kafkajs';

interface KafkaConsumerConfig {
  clientId: string;
  groupId: string;
  brokers: string[];
  ssl?: boolean;
  sasl?: {
    mechanism: 'plain' | 'scram-sha-256' | 'scram-sha-512';
    username: string;
    password: string;
  };
  sessionTimeout?: number;
  heartbeatInterval?: number;
  maxBytesPerPartition?: number;
  minBytes?: number;
  maxBytes?: number;
  maxWaitTimeInMs?: number;
}

interface ConsumerMetrics {
  messagesConsumed: Counter<string>;
  consumerLatency: Histogram<string>;
  consumerErrors: Counter<string>;
  processingTime: Histogram<string>;
}

class KafkaConsumerService {
  private kafka: Kafka;
  private consumer: Consumer;
  private logger = createLogger({ level: 'info' });
  private metrics: ConsumerMetrics;
  private isConnected = false;
  private messageHandlers = new Map<string, (message: any) => Promise<void>>();

  constructor(private config: KafkaConsumerConfig) {
    this.kafka = new Kafka({
      clientId: config.clientId,
      brokers: config.brokers,
      ssl: config.ssl,
      sasl: config.sasl,
    });

    this.consumer = this.kafka.consumer({
      groupId: config.groupId,
      sessionTimeout: config.sessionTimeout || 30000,
      heartbeatInterval: config.heartbeatInterval || 3000,
      maxBytesPerPartition: config.maxBytesPerPartition || 1048576,
      minBytes: config.minBytes || 1,
      maxBytes: config.maxBytes || 10485760,
      maxWaitTimeInMs: config.maxWaitTimeInMs || 5000,
      allowAutoTopicCreation: false,
      retry: {
        retries: 8,
        initialRetryTime: 100,
        maxRetryTime: 30000,
      },
    });

    this.initializeMetrics();
    this.setupErrorHandling();
  }

  private initializeMetrics(): void {
    this.metrics = {
      messagesConsumed: new Counter({
        name: 'kafka_messages_consumed_total',
        help: 'Total number of messages consumed',
        labelNames: ['topic', 'partition', 'status'],
      }),
      consumerLatency: new Histogram({
        name: 'kafka_consumer_latency_seconds',
        help: 'Consumer latency in seconds',
        labelNames: ['topic'],
        buckets: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5],
      }),
      consumerErrors: new Counter({
        name: 'kafka_consumer_errors_total',
        help: 'Total number of consumer errors',
        labelNames: ['topic', 'error_type'],
      }),
      processingTime: new Histogram({
        name: 'kafka_message_processing_seconds',
        help: 'Message processing time in seconds',
        labelNames: ['topic', 'handler'],
        buckets: [0.001, 0.01, 0.1, 1, 10, 60],
      }),
    };

    register.registerMetric(this.metrics.messagesConsumed);
    register.registerMetric(this.metrics.consumerLatency);
    register.registerMetric(this.metrics.consumerErrors);
    register.registerMetric(this.metrics.processingTime);
  }

  private setupErrorHandling(): void {
    this.consumer.on('consumer.connect', () => {
      this.logger.info('Consumer connected');
      this.isConnected = true;
    });

    this.consumer.on('consumer.disconnect', () => {
      this.logger.warn('Consumer disconnected');
      this.isConnected = false;
    });

    this.consumer.on('consumer.crash', (payload) => {
      this.logger.error('Consumer crashed', payload);
      this.metrics.consumerErrors.inc({ topic: 'unknown', error_type: 'crash' });
    });
  }

  async connect(): Promise<void> {
    try {
      await this.consumer.connect();
      this.logger.info('Kafka consumer connected successfully');
    } catch (error) {
      this.logger.error('Failed to connect consumer', error);
      throw error;
    }
  }

  async disconnect(): Promise<void> {
    try {
      await this.consumer.disconnect();
      this.logger.info('Kafka consumer disconnected successfully');
    } catch (error) {
      this.logger.error('Failed to disconnect consumer', error);
      throw error;
    }
  }

  registerHandler<T>(
    topic: string,
    handler: (message: T, metadata: MessageMetadata) => Promise<void>
  ): void {
    this.messageHandlers.set(topic, handler);
    this.logger.info(`Registered handler for topic: ${topic}`);
  }

  async subscribe(topics: string[]): Promise<void> {
    try {
      await this.consumer.subscribe({ 
        topics,
        fromBeginning: false,
      });
      
      await this.consumer.run({
        eachMessage: this.handleMessage.bind(this),
        partitionsConsumedConcurrently: 3,
      });

      this.logger.info('Consumer subscribed to topics', { topics });
    } catch (error) {
      this.logger.error('Failed to subscribe to topics', error);
      throw error;
    }
  }

  private async handleMessage(payload: EachMessagePayload): Promise<void> {
    const { topic, partition, message } = payload;
    const startTime = Date.now();
    
    try {
      if (!message.value) {
        this.logger.warn('Received message with no value', { topic, partition });
        return;
      }

      const messageContent = JSON.parse(message.value.toString());
      const { data, metadata } = messageContent;

      const handler = this.messageHandlers.get(topic);
      if (!handler) {
        this.logger.warn(`No handler registered for topic: ${topic}`);
        return;
      }

      const processingStart = Date.now();
      
      await handler(data, metadata);
      
      const processingTime = (Date.now() - processingStart) / 1000;
      const totalLatency = (Date.now() - startTime) / 1000;

      // Update metrics
      this.metrics.messagesConsumed.inc({ 
        topic, 
        partition: partition.toString(), 
        status: 'success' 
      });
      this.metrics.consumerLatency.observe({ topic }, totalLatency);
      this.metrics.processingTime.observe({ topic, handler: 'default' }, processingTime);

      this.logger.info('Message processed successfully', {
        topic,
        partition,
        offset: message.offset,
        messageId: metadata.messageId,
        processingTime,
        totalLatency,
      });

    } catch (error) {
      const latency = (Date.now() - startTime) / 1000;
      
      this.metrics.consumerErrors.inc({ 
        topic, 
        error_type: error.name || 'unknown' 
      });
      
      this.logger.error('Failed to process message', {
        topic,
        partition,
        offset: message.offset,
        error: error.message,
        latency,
      });
      
      // Don't throw - let Kafka handle retry logic
    }
  }

  // Health check method
  async healthCheck(): Promise<{ status: string; connected: boolean }> {
    return {
      status: this.isConnected ? 'healthy' : 'unhealthy',
      connected: this.isConnected,
    };
  }
}

// Usage example
const producerConfig: KafkaProducerConfig = {
  clientId: 'my-app-producer',
  brokers: ['kafka-1:9092', 'kafka-2:9092', 'kafka-3:9092'],
  ssl: true,
  sasl: {
    mechanism: 'scram-sha-256',
    username: process.env.KAFKA_USERNAME!,
    password: process.env.KAFKA_PASSWORD!,
  },
  idempotent: true,
  compression: CompressionTypes.GZIP,
};

const consumerConfig: KafkaConsumerConfig = {
  clientId: 'my-app-consumer',
  groupId: 'my-app-group',
  brokers: ['kafka-1:9092', 'kafka-2:9092', 'kafka-3:9092'],
  ssl: true,
  sasl: {
    mechanism: 'scram-sha-256',
    username: process.env.KAFKA_USERNAME!,
    password: process.env.KAFKA_PASSWORD!,
  },
};

export { KafkaProducerService, KafkaConsumerService };]]></correct-example>
          <incorrect-example title="Basic producer/consumer without error handling" conditions="Implementing Kafka client" expected-result="Robust producer/consumer with proper error handling" incorrectness-criteria="No error handling, no retry logic, no monitoring, no idempotency"><![CDATA[// Basic Kafka producer without error handling
import { Kafka } from 'kafkajs';

const kafka = new Kafka({
  clientId: 'my-app',
  brokers: ['localhost:9092'],
});

const producer = kafka.producer();
const consumer = kafka.consumer({ groupId: 'my-group' });

// Bad: No error handling
await producer.connect();
await producer.send({
  topic: 'my-topic',
  messages: [{ value: 'Hello World' }],
});

// Bad: No error handling or retry logic
await consumer.connect();
await consumer.subscribe({ topic: 'my-topic' });

await consumer.run({
  eachMessage: async ({ message }) => {
    console.log(message.value.toString());
    // Bad: No error handling
    // Bad: No monitoring
    // Bad: No message validation
  },
});

// Bad: No metrics
// Bad: No structured logging
// Bad: No health checks
// Bad: No graceful shutdown]]></incorrect-example>
        </example>
      </examples>
    </requirement>

    <requirement priority="high">
      <description>Design proper topic architecture with appropriate partitioning strategies, retention policies, compaction settings, and schema evolution for scalable and maintainable event-driven systems.</description>
      <examples>
        <example title="Kafka Topic Design and Management">
          <correct-example title="Comprehensive topic design with proper configuration" conditions="Designing Kafka topics for event-driven architecture" expected-result="Well-designed topics with proper partitioning and retention" correctness-criteria="Proper partitioning, retention policies, schema management, monitoring"><![CDATA[#!/bin/bash
# kafka-topic-management.sh - Comprehensive topic management

set -euo pipefail

KAFKA_BROKERS="kafka-1:9092,kafka-2:9092,kafka-3:9092"
KAFKA_ADMIN_CONFIG="--bootstrap-server ${KAFKA_BROKERS}"

echo "🚀 Creating Kafka topics with proper configuration"

# Function to create topic with proper configuration
create_topic() {
  local topic_name=$1
  local partitions=$2
  local replication_factor=$3
  local retention_ms=$4
  local segment_ms=$5
  local cleanup_policy=$6
  local min_insync_replicas=$7
  
  echo "Creating topic: ${topic_name}"
  
  kafka-topics.sh ${KAFKA_ADMIN_CONFIG} \
    --create \
    --topic "${topic_name}" \
    --partitions "${partitions}" \
    --replication-factor "${replication_factor}" \
    --config retention.ms="${retention_ms}" \
    --config segment.ms="${segment_ms}" \
    --config cleanup.policy="${cleanup_policy}" \
    --config min.insync.replicas="${min_insync_replicas}" \
    --config compression.type=gzip \
    --config max.message.bytes=1048576 \
    --config segment.bytes=1073741824 \
    --if-not-exists
}

# Event sourcing topics - Long retention, high partitions
create_topic "user.events" 24 3 604800000 3600000 "delete" 2  # 7 days retention
create_topic "order.events" 24 3 2592000000 3600000 "delete" 2  # 30 days retention
create_topic "payment.events" 12 3 7776000000 3600000 "delete" 2  # 90 days retention

# State topics - Compacted for latest state
create_topic "user.state" 12 3 -1 3600000 "compact" 2  # Infinite retention
create_topic "inventory.state" 8 3 -1 3600000 "compact" 2
create_topic "catalog.state" 8 3 -1 3600000 "compact" 2

# Command topics - Short retention for processing
create_topic "user.commands" 12 3 86400000 1800000 "delete" 2  # 1 day retention
create_topic "order.commands" 12 3 86400000 1800000 "delete" 2
create_topic "payment.commands" 8 3 86400000 1800000 "delete" 2

# Integration topics - Medium retention
create_topic "external.webhooks" 6 3 259200000 3600000 "delete" 2  # 3 days retention
create_topic "notifications.email" 4 3 259200000 3600000 "delete" 2
create_topic "notifications.sms" 4 3 259200000 3600000 "delete" 2

# Dead letter topics - Long retention for debugging
create_topic "dlq.user.events" 4 3 2592000000 3600000 "delete" 2  # 30 days retention
create_topic "dlq.order.events" 4 3 2592000000 3600000 "delete" 2
create_topic "dlq.payment.events" 4 3 2592000000 3600000 "delete" 2

# Schema Registry topics (managed by Confluent)
create_topic "_schemas" 1 3 -1 3600000 "compact" 2

echo "✅ Topics created successfully"

# Avro schema definitions for strong typing
cat > user-event-schema.avsc <<EOF
{
  "type": "record",
  "name": "UserEvent",
  "namespace": "com.myapp.events",
  "fields": [
    {
      "name": "eventId",
      "type": "string",
      "doc": "Unique identifier for the event"
    },
    {
      "name": "eventType",
      "type": {
        "type": "enum",
        "name": "UserEventType",
        "symbols": ["USER_CREATED", "USER_UPDATED", "USER_DELETED", "USER_LOGIN", "USER_LOGOUT"]
      },
      "doc": "Type of user event"
    },
    {
      "name": "userId",
      "type": "string",
      "doc": "User identifier"
    },
    {
      "name": "timestamp",
      "type": "long",
      "logicalType": "timestamp-millis",
      "doc": "Event timestamp in milliseconds"
    },
    {
      "name": "version",
      "type": "int",
      "default": 1,
      "doc": "Schema version"
    },
    {
      "name": "metadata",
      "type": {
        "type": "record",
        "name": "EventMetadata",
        "fields": [
          {"name": "source", "type": "string"},
          {"name": "correlationId", "type": ["null", "string"], "default": null},
          {"name": "causationId", "type": ["null", "string"], "default": null}
        ]
      }
    },
    {
      "name": "payload",
      "type": {
        "type": "record",
        "name": "UserEventPayload",
        "fields": [
          {"name": "email", "type": ["null", "string"], "default": null},
          {"name": "name", "type": ["null", "string"], "default": null},
          {"name": "status", "type": ["null", "string"], "default": null},
          {"name": "previousValues", "type": ["null", {"type": "map", "values": "string"}], "default": null}
        ]
      }
    }
  ]
}
EOF

cat > order-event-schema.avsc <<EOF
{
  "type": "record",
  "name": "OrderEvent",
  "namespace": "com.myapp.events",
  "fields": [
    {
      "name": "eventId",
      "type": "string"
    },
    {
      "name": "eventType",
      "type": {
        "type": "enum",
        "name": "OrderEventType",
        "symbols": ["ORDER_CREATED", "ORDER_UPDATED", "ORDER_CANCELLED", "ORDER_COMPLETED", "ORDER_SHIPPED"]
      }
    },
    {
      "name": "orderId",
      "type": "string"
    },
    {
      "name": "userId",
      "type": "string"
    },
    {
      "name": "timestamp",
      "type": "long",
      "logicalType": "timestamp-millis"
    },
    {
      "name": "version",
      "type": "int",
      "default": 1
    },
    {
      "name": "metadata",
      "type": {
        "type": "record",
        "name": "EventMetadata",
        "fields": [
          {"name": "source", "type": "string"},
          {"name": "correlationId", "type": ["null", "string"], "default": null},
          {"name": "causationId", "type": ["null", "string"], "default": null}
        ]
      }
    },
    {
      "name": "payload",
      "type": {
        "type": "record",
        "name": "OrderEventPayload",
        "fields": [
          {"name": "totalAmount", "type": ["null", "double"], "default": null},
          {"name": "currency", "type": ["null", "string"], "default": null},
          {"name": "status", "type": ["null", "string"], "default": null},
          {"name": "items", "type": ["null", {"type": "array", "items": "string"}], "default": null},
          {"name": "shippingAddress", "type": ["null", "string"], "default": null}
        ]
      }
    }
  ]
}
EOF

# Register schemas with Schema Registry
echo "📋 Registering schemas with Schema Registry"

# Function to register schema
register_schema() {
  local subject=$1
  local schema_file=$2
  
  echo "Registering schema for subject: ${subject}"
  
  curl -X POST \
    -H "Content-Type: application/vnd.schemaregistry.v1+json" \
    --data "{\"schema\":\"$(cat ${schema_file} | jq -c . | sed 's/"/\\"/g')\"}" \
    http://schema-registry:8081/subjects/${subject}/versions
}

register_schema "user.events-value" "user-event-schema.avsc"
register_schema "order.events-value" "order-event-schema.avsc"

# Topic monitoring and alerting configuration
cat > topic-monitoring.yml <<EOF
groups:
- name: kafka.topics
  rules:
  - alert: KafkaTopicUnderReplicated
    expr: kafka_topic_partition_under_replicated_partition > 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Kafka topic {{ \$labels.topic }} is under-replicated"
      description: "Topic {{ \$labels.topic }} has {{ \$value }} under-replicated partitions"

  - alert: KafkaTopicHighLag
    expr: kafka_consumer_lag_sum{topic!~"^_.*"} > 1000
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "High consumer lag on topic {{ \$labels.topic }}"
      description: "Consumer group {{ \$labels.consumergroup }} has lag of {{ \$value }} on topic {{ \$labels.topic }}"

  - alert: KafkaTopicLowThroughput
    expr: rate(kafka_topic_partition_current_offset[5m]) < 0.1
    for: 15m
    labels:
      severity: info
    annotations:
      summary: "Low throughput on topic {{ \$labels.topic }}"
      description: "Topic {{ \$labels.topic }} has low message throughput"

  - alert: KafkaTopicRetentionWarning
    expr: (kafka_log_size_bytes / kafka_log_retention_bytes) > 0.9
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Topic {{ \$labels.topic }} approaching retention limit"
      description: "Topic {{ \$labels.topic }} is at {{ \$value | humanizePercentage }} of retention limit"
EOF

echo "✅ Topic management completed successfully"
echo "📊 Topics created with proper partitioning and retention policies"
echo "📋 Schemas registered with Schema Registry"
echo "🔍 Monitoring and alerting configured"]]></correct-example>
          <incorrect-example title="Basic topic creation without proper design" conditions="Creating Kafka topics" expected-result="Well-designed topics with proper configuration" incorrectness-criteria="Poor partitioning, no retention policies, no schema management, no monitoring"><![CDATA[#!/bin/bash
# Basic topic creation without proper design

kafka-topics.sh --create --topic my-topic --bootstrap-server localhost:9092

# Bad: No partition specification
# Bad: No replication factor
# Bad: No retention policies
# Bad: No schema management
# Bad: No monitoring
# Bad: No naming conventions
# Bad: Single topic for everything]]></incorrect-example>
        </example>
      </examples>
    </requirement>
  </requirements>
  <context description="Apache Kafka distributed streaming platform best practices">
    Apache Kafka is a distributed streaming platform that excels at handling high-throughput, real-time data feeds. Modern Kafka development emphasizes proper cluster design, security-first approaches, comprehensive monitoring, and robust producer/consumer patterns for reliable event-driven architectures.

    Key principles for Kafka implementation include:
    - Cluster design with proper replication, partitioning, and high availability
    - Security with authentication, authorization, encryption, and network isolation
    - Producer/consumer patterns with idempotency, exactly-once semantics, and error handling
    - Topic design with appropriate partitioning strategies and retention policies
    - Schema management with Schema Registry for data governance and evolution
    - Comprehensive monitoring and alerting for cluster health and performance

    Kafka security is critical and includes SASL authentication (PLAIN, SCRAM, OAuth), SSL/TLS encryption for data in transit, ACLs for authorization, and proper network security. All clusters should use secure configurations by default.

    Performance optimization involves proper partitioning for parallelism, compression to reduce network overhead, batching for throughput, and appropriate resource allocation. Monitoring should cover broker health, topic metrics, consumer lag, and application-level metrics.

    Production deployments require disaster recovery strategies, backup and restore procedures, capacity planning, and integration with monitoring and alerting systems. Modern Kafka ecosystems often include Schema Registry for data governance, Kafka Connect for integration, and KSQL for stream processing.

    Event-driven architectures with Kafka enable microservices communication, real-time analytics, data integration, and event sourcing patterns, making it essential for modern distributed systems and cloud-native applications.
  </context>
  <references>
    <reference as="dependency" href=".cursor/rules/000-core/002-cursor-rules-creation.mdc" reason="Follows standard rule format">Base rule format definition</reference>
    <reference as="context" href="https://kafka.apache.org/documentation/" reason="Official Kafka documentation">Apache Kafka Documentation</reference>
    <reference as="context" href="https://docs.confluent.io/platform/current/security/index.html" reason="Security best practices">Confluent Security Guide</reference>
    <reference as="context" href="https://kafka.apache.org/documentation/#bestpractices" reason="Performance and operational best practices">Kafka Best Practices</reference>
  </references>
</rule>
